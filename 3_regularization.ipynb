{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmnist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the logistic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-64-2c4cf34116f5>:6 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 19.476709\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 13.3%\n",
      "Minibatch loss at step 500: 3.235711\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 1000: 1.567439\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 1500: 1.021430\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 2000: 0.788871\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 2500: 0.906093\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 3000: 0.872816\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 3500: 0.651090\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 4000: 0.511519\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 4500: 0.558165\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 5000: 0.476784\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 5500: 0.839810\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 6000: 0.801831\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 6500: 0.715732\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 7000: 0.789617\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 7500: 0.689137\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 8000: 0.749962\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 8500: 0.876629\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 9000: 0.618728\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 9500: 0.600716\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 10000: 0.780825\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 10500: 0.600508\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 11000: 0.611721\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 11500: 0.666230\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 12000: 0.658829\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 12500: 0.656589\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 13000: 0.650762\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 13500: 0.612823\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 14000: 0.609619\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 14500: 0.861195\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 15000: 0.840222\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 15500: 0.509230\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 16000: 0.791400\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 16500: 0.666940\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 17000: 0.586182\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 17500: 0.614865\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 18000: 0.625612\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 18500: 0.703361\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 19000: 0.557485\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 19500: 0.466888\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 20000: 0.621014\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 88.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "# add learning curve \n",
    "learning_val = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      accuracy_train = accuracy(predictions, batch_labels)\n",
    "      accuracy_val = accuracy(valid_prediction.eval(), valid_labels)\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy_train)\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy_val)\n",
    "    if (step%50 ==0):\n",
    "      accuracy_val = accuracy(valid_prediction.eval(), valid_labels)\n",
    "      learning_val.append(accuracy_val)\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAF4CAYAAAA1w9ECAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKVEQERGRopQoiIiISFH/HyyF6RTCTCuoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f267bc00490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate x Axis\n",
    "step_x = range (len(learning_val))\n",
    "plt.semilogx(step_x, learning_val)\n",
    "plt.grid(True)\n",
    "plt.title('Learning rate, x axis, every 50 steps, y accuracy validating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2 regularization introduces a new meta parameter that should be tuned. Since I do not have any idea of what should be the right value for this meta parameter, I will plot the accuracy by the meta parameter value (in a logarithmic scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-66-fa55255dcf2f>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-66-fa55255dcf2f>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-66-fa55255dcf2f>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-66-fa55255dcf2f>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-66-fa55255dcf2f>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-66-fa55255dcf2f>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the same technique will improve the prediction of the 1-layer neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-b4636e904658>:4 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 577.955261\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 22.0%\n",
      "Minibatch loss at step 500: 195.523834\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 1000: 113.152191\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 1500: 68.121567\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 2000: 41.150600\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 2500: 25.203238\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 3000: 15.384150\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.0%\n",
      "Test accuracy: 92.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally something above 90%! I will also plot the final accuracy by the L2 parameter to find the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-ab3df16a8411>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:    \n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAF4CAYAAAA1w9ECAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFgqSJGmg/wEk+f+xf0X/OgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26c20dfa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-5989025b082e>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 294.559601\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 39.5%\n",
      "Minibatch loss at step 2: 859.750610\n",
      "Minibatch accuracy: 50.8%\n",
      "Validation accuracy: 37.8%\n",
      "Minibatch loss at step 4: 164.810898\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 58.1%\n",
      "Minibatch loss at step 6: 3.282358\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 8: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 10: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 12: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 14: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 16: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 18: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 22: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 28: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Test accuracy: 75.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_bacthes = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_bacthes\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are far too much parameters and no regularization, the accuracy of the batches is 100%. The generalization capability is poor, as shown in the validation and test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-412275055f98>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 488.475067\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 32.4%\n",
      "Minibatch loss at step 2: 1032.497925\n",
      "Minibatch accuracy: 45.3%\n",
      "Validation accuracy: 46.2%\n",
      "Minibatch loss at step 4: 80.057861\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 64.6%\n",
      "Minibatch loss at step 6: 7.130970\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 65.9%\n",
      "Minibatch loss at step 8: 16.144968\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 10: 11.340226\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 12: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 14: 6.176638\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 16: 2.788801\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 18: 0.523599\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 20: 1.779016\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 22: 1.232905\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 26: 1.019791\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 28: 4.295843\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 30: 2.076526\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 36: 0.000014\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 38: 0.310153\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 42: 0.023768\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.7%\n",
      "Minibatch loss at step 44: 0.019014\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 48: 0.310937\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.1%\n",
      "Minibatch loss at step 50: 0.695106\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.4%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.4%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 56: 3.677459\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 60: 0.000977\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 62: 0.073986\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.9%\n",
      "Test accuracy: 75.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first conclusion is that 100% of accuracy on the minibatches is more difficult achieved or to keep. As a result, the test accuracy is improved by 6%, the final net is more capable of generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a first try with 2 layers. Note how the parameters are initialized, compared to the previous cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-9e6c84e830a7>:4 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 3.233874\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 43.1%\n",
      "Minibatch loss at step 500: 1.169163\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 1000: 0.691700\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 1500: 0.594109\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 2000: 0.537101\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 2500: 0.633098\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 3000: 0.581091\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 3500: 0.444799\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 4000: 0.372816\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 4500: 0.410046\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 5000: 0.409083\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 5500: 0.486362\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 6000: 0.523667\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 6500: 0.471178\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 7000: 0.481775\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 7500: 0.360593\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 8000: 0.424267\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 8500: 0.488855\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 9000: 0.455198\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.5%\n",
      "Test accuracy: 95.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting really good. Let's try one layer deeper with dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-a2ea4199a64b>:4 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 2.358727\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 38.2%\n",
      "Minibatch loss at step 500: 0.578984\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1000: 0.309503\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 1500: 0.284852\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 2000: 0.261055\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 2500: 0.424393\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 3000: 0.332761\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 3500: 0.221629\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 4000: 0.178431\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 4500: 0.215702\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 5000: 0.185262\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 5500: 0.222498\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 6000: 0.241916\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 6500: 0.303368\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 7000: 0.182052\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 7500: 0.113996\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 8000: 0.185021\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 8500: 0.241721\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 9000: 0.179077\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 9500: 0.142594\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 10000: 0.109250\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 10500: 0.127757\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 11000: 0.102513\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 11500: 0.101140\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 12000: 0.145797\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 12500: 0.107367\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 13000: 0.055700\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 13500: 0.073788\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 14000: 0.079020\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 14500: 0.110245\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 15000: 0.095435\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 15500: 0.083509\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 16000: 0.092399\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 16500: 0.078605\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 17000: 0.040912\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 17500: 0.023058\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 18000: 0.032806\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.9%\n",
      "Test accuracy: 96.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huge! That's my best score on this dataset. I have also tried more parameters, but it does not help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 256\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "  drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "  drop3 = tf.nn.dropout(lay3_train, 0.5)\n",
    "  logits = tf.matmul(drop3, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 5000, 0.80, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-26-9de5e9efb445>:4 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 2.833015\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 14.6%\n",
      "Minibatch loss at step 500: 0.755550\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 1000: 0.485251\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 1500: 0.462174\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 2000: 0.378567\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 2500: 0.589194\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 3000: 0.496328\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 3500: 0.433389\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 4000: 0.307377\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 4500: 0.396830\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 5000: 0.422105\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 5500: 0.548971\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 6000: 0.499068\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 6500: 0.414204\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 7000: 0.435063\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 7500: 0.328551\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 8000: 0.422736\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 8500: 0.503347\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 9000: 0.381223\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 9500: 0.375834\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 10000: 0.387934\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 10500: 0.388146\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 11000: 0.308032\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 11500: 0.398565\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 12000: 0.317936\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 12500: 0.460312\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 13000: 0.351738\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 13500: 0.379695\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 14000: 0.345551\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 14500: 0.409537\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 15000: 0.374108\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 15500: 0.229769\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 16000: 0.357662\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 16500: 0.457958\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 17000: 0.261216\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 17500: 0.319901\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 18000: 0.322088\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 18500: 0.369225\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 19000: 0.225751\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 19500: 0.401470\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 20000: 0.329025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Test accuracy: 95.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
